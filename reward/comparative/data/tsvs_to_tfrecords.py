from reward.comparative.data.ops import DATASET_IS_PACKED
import sys
from absl import flags
from functools import partial

import tensorflow.compat.v1 as tf
from mesh_tensorflow.transformer.dataset import trim_and_pad_dataset

from data.to_tfrecord_t5 import encoder as TOKENIZER
from t5.data.utils import encode_string_features
from reward.comparative.data import \
    TSV_PATH, TSV_COLNAMES, SPLITS, SEQUENCE_LENGTH, LOCAL_TFRECORDS_PATH, \
    DATASET_IS_PACKED

def _define_flags():
    flags.DEFINE_string(
        name="dataset_id",
        default=None,
        help="Id of the dataset generated by jsonl_to_tsvs.py"
    )
    return flags.FLAGS

def _add_position_and_segmentation(sample):
    """
    These tensors are generated by mtf.transformer.dataset.pack_or_pad with
    pack=True. We're not supporting packing, so we have to add them manually
    for mtf transformers to work.
    """
    new_items = {}
    for k, v in sample.items():
        new_items[k + "_segmentation"] = (v * 0) + 1
        new_items[k + "_position"] = tf.range(len(v))
    sample.update(new_items)
    return sample

def _serialize_tokens(*values, colnames):
    example_data = {
        colnames[i]: tf.train.Feature(
            int64_list=tf.train.Int64List(value=tokens)
        )
        for i, tokens in enumerate(values)
    }
    example = tf.train.Example(features=tf.train.Features(feature=example_data))
    return example.SerializeToString()

def _tf_serialize_tokens(tf_tokens_dict):
    serialize_with_colnames = partial(
        _serialize_tokens,
        colnames=list(tf_tokens_dict.keys())
    )
    tf_string = tf.py_function(
        func=serialize_with_colnames,
        inp=list(tf_tokens_dict.values()),
        Tout=tf.string
    )
    return tf.reshape(tf_string, ())

if __name__ == "__main__":
    tf.enable_eager_execution()
    assert (not DATASET_IS_PACKED), "Packed dataset not supported"
    FLAGS = _define_flags()
    FLAGS(sys.argv)
    for split in SPLITS:
        tsv_path = TSV_PATH.format(dataset_id=FLAGS.dataset_id, split=split)
        tsv_dataset = tf.data.experimental.CsvDataset(
            tsv_path,
            record_defaults=["", "", ""],
            field_delim="\t",
            use_quote_delim=False
        )
        tsv_dataset = tsv_dataset.map(
            lambda *x: {c: x[i] for i, c in enumerate(TSV_COLNAMES)}
        )
        tokens_dataset = encode_string_features(
            dataset=tsv_dataset,
            vocabulary=TOKENIZER,   
            copy_plaintext=False,
            keys=TSV_COLNAMES
        )
        unpadded_dataset = tokens_dataset.map(_add_position_and_segmentation)
        SEQUENCE_LENGTH.update({
            "targets1": SEQUENCE_LENGTH["targets"],
            "targets2": SEQUENCE_LENGTH["targets"],
        })
        _sequence_length = {
            **SEQUENCE_LENGTH,
            **{k + "_position": v for k, v in SEQUENCE_LENGTH.items()},
            **{k + "_segmentation": v for k, v in SEQUENCE_LENGTH.items()}
        }
        padded_dataset = trim_and_pad_dataset(
            dataset=unpadded_dataset,
            length=_sequence_length
        )
        serialized_dataset = padded_dataset.map(_tf_serialize_tokens)
        tfrecords_path = LOCAL_TFRECORDS_PATH.format(
            dataset_id=FLAGS.dataset_id,
            split=split
        )
        # Shuffle dataset
        shuffled_dataset = serialized_dataset.shuffle(
            buffer_size = 10000,
            seed = 41
        )
        # Write tfrecords dataset to disk
        print(f"Writting '{split}' tfrecords")
        tfrecords_writer = tf.data.experimental.TFRecordWriter(tfrecords_path)
        tfrecords_writer.write(shuffled_dataset)